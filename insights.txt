24/08
Parece haver um caso em que ele ainda não consegue encontrar
uma solução.
Casos em que tem que retornar um quadrado. 

Preciso padronizar a forma de registrar os resultados dos meus
testes.

Preciso gerar mais gráficos. Formas de medir o desempenho
dos testes. (Criar uma planilha)

Criar melhor visualização para o drone. 

Considerar retornar para o Pygame e criar uma interface?
Talvez pygame não seja a melhor opção, mas gostaria de algo gráfico para
usar como interface. Uma aplicação completa.

Mapping - SLAM
vou precisar gerar uma estrutura de dados para isso

verificar métricas que já estou usando:
loss, média dos rewards, average episode length, average of episodes

no texto eu posso usar uma sessão para explicar a forma como o TF-Agents 
funciona: ciclo de coleta de dados, ciclo de avaliação, ciclo de 
treinamento.

entender melhor as interfaces de dados. Como obter dados de dentro do 
treinamento. Como gerar métricas. ETC.

gerar testes extensivos que coletem dados para verificar o efeito de 
algumas modificações no conjunto de variáveis.
e realizar esses testes em vários cenários: com e sem paredes. Com 
mais ou menos informações. Usar esses dados no texto.

Lembrar do método científico: problema > hipótese > validar hipótese.

rever passos e problemas pelos quais passei e tentar encaixar isso de 
alguma forma no texto.

preciso rever as orientações sobre escrita científica que já me esqueci

alguma função para apenas rodar o modelo treinado, verificar métricas 
a partir disso

visual impressiona. Talvez eu devesse investir nisso. gerar simulações
mais visualmente bonitas.

salvar agente treinado.

filtrar arquivos. Tem muitos

SOBRE ARQUIVOS ABERTOS:
training copy 3: apenas um estágio, sem paredes
training copy 4: dois estágios, os dois com paredes. Não funcionou muito
bem. O primeiro estágio apresenta flickering. O segundo melhora os 
resultados, mas ainda apresenta o mesmo comportamento.
training copy 5: dois estágios, um com parede e o outro sem. Não sei o
que mudei dessa tentativa para a próxima, mas esse apresenta flickering.
O último aberto: training copy 6, parece ter o treinamento separado em
estágios.

preciso entender se houve diferenças entre os dois últimos (copy 5 e
copy 6), a fim de entender se o processo é reproduzível. 


06/09
realizar diversos treinamentos. Verificar a média de episódios que leva 
para o modelo convergir. Verificar se há algum desvio muito grande.

Métricas, gráficos e log de resultados são importantes.


Onde eu estou?
Eu tenho um notebook que consegue treinar um agente para se movimentar
em um labirinto de 3x3. O treinamento segue algumas etapas. Não sei
dizer quantas das vezes o agente é bem sucedido em suas tentativas
(faltam métricas), mas sei que ele falha algumas vezes.

Tenho muitas ideias do que melhorar. (Preciso focar no essencial).

Tenho um método que funciona. Ele precisa ser robusto.


TESTAR ROBUSTEZ:
Para isso é necessário:
- Testes repetitivos 
- Métricas

Medir:
Quantas vezes o drone atinge uma parede?
Quantas vezes ele não é capaz de completar o labirinto?
Qual a média de passos ele leva pra completar?
loss (ele ainda está aprendendo algo?)
Média de Reward por episódio

Método:
Rodar o treinamento 100 vezes com x episódios (e aumentar o número de
episódios gradativamente).
Verificar as métricas.
Gerar um histórico, para aferir qual o número ótimo de episódios
necessário para ensiar o agente aquela habilidade.


09/09
Verificar métricas usadas
Padronizar forma de registrar resultados dos testes
Entender melhor interface dos dados
Gerar testes extensivos que coletem dados
Método para aferir a robustez

cenário do experimento:
rodar 4 treinamentos sem paredes
- somente com recompensa positiva
- recompensa positiva e negativa, porém negativa menor do que positiva
- recompensa positiva e negativa, de igual magnitude
- recompensa positiva e negativa, porém negativa maior do que positiva 
acompanhar loss junto

fazer o teste de crash em ambiente jogável

21/09 
Criar subplot que plote todos os gráficos na mesma linha. Facilitará
a comparação de resultados. 

