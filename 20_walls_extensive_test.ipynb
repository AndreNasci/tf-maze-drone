{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MazeDrone with TF Agents - 20 Check agent performance with Walls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "\n",
    "from resources import build_agent, TrainingSession\n",
    "import pandas as pd\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_combination = 1\n",
    "min_run = 1\n",
    "max_run = 1\n",
    "\n",
    "\n",
    "num_iterations = 20_000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 64  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "#replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 100  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 100  # @param {type:\"integer\"}\n",
    "\n",
    "# Agent fully connected layer params \n",
    "fc_layer_params = (200,) \n",
    "\n",
    "# File's name\n",
    "description = \"201\"\n",
    "\n",
    "maze_size = 3\n",
    "\n",
    "rewards = []\n",
    "rewards.append({\n",
    "    'destroyed': -10.,\n",
    "    'stuck': - 6.,\n",
    "    'reached': 10.,\n",
    "    'standard': -1.\n",
    "})\n",
    "rewards.append({\n",
    "    'destroyed': -10.,\n",
    "    'stuck': -11.,\n",
    "    'reached': 10.,\n",
    "    'standard': -1.\n",
    "})\n",
    "rewards.append({\n",
    "    'destroyed': -10.,\n",
    "    'stuck': -15.,\n",
    "    'reached': 10.,\n",
    "    'standard': -1.\n",
    "})\n",
    "\n",
    "\n",
    "sys.path.append('/home/naski/Documents/dev/maze_drone_v02')\n",
    "import gym_maze # Esta linha precisa estar após o PATH\n",
    "\n",
    "# Importing custom environment\n",
    "env_name = 'maze-v0'\n",
    "env = suite_gym.load(env_name)\n",
    "\n",
    "# Testing\n",
    "env.reset()\n",
    "\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "# Converts environments, originally in pure Python, to tensors (using a wrapper)\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CREATING/RESETING THE AGENT\n",
    "agent = build_agent(fc_layer_params, env, learning_rate, train_env)\n",
    "agent.initialize()\n",
    "\n",
    "# GENERATE TRAINING SESSION\n",
    "session = TrainingSession(description, maze_size, env_name, rewards[0], agent, collect_steps_per_iteration, \n",
    "                        1, eval_interval, replay_buffer_max_length, num_eval_episodes)\n",
    "\n",
    "# TRAINING\n",
    "step_log, returns, finished, crashed, stucked, steped, _, replay_buffer, _ = session.train()\n",
    "\n",
    "# CLEAR MEMORY\n",
    "del(session)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"============================================================== After first training:\")\n",
    "\n",
    "print(agent.train_step_counter)\n",
    "\n",
    "checkpoint_dir = './checkpoint/phase-2-1-stuck'\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=agent.train_step_counter\n",
    ")\n",
    "\n",
    "print(agent.train_step_counter)\n",
    "\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "    \n",
    "print(agent.train_step_counter)\n",
    "\n",
    "agent._optimizer.learning_rate = learning_rate\n",
    "print(agent._optimizer.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========================================================================== Training:\")\n",
    "\n",
    "\n",
    "for combination in range(min_combination-1, min_combination):\n",
    "\n",
    "    for run in range(min_run-1, max_run):\n",
    "        print('Combination', combination + 1, ' | Run', run + 1)\n",
    "        train_checkpointer = common.Checkpointer(\n",
    "            ckpt_dir=checkpoint_dir,\n",
    "            max_to_keep=1,\n",
    "            agent=agent,\n",
    "            policy=agent.policy,\n",
    "            replay_buffer=replay_buffer,\n",
    "            global_step=agent.train_step_counter\n",
    "        )\n",
    "        agent._optimizer.learning_rate = learning_rate\n",
    "        print(agent.train_step_counter)\n",
    "\n",
    "\n",
    "        # GENERATE TRAINING SESSION\n",
    "        session = TrainingSession(description, maze_size, env_name, rewards[combination], agent, collect_steps_per_iteration, \n",
    "                                num_iterations, eval_interval, replay_buffer_max_length, num_eval_episodes)\n",
    "        \n",
    "        # TRAINING\n",
    "        step_log, returns, finished, crashed, stucked, steped, log_loss, _, agent = session.train(without_wall_training=False, early_stop=\"stuckANDcrash\")\n",
    "\n",
    "        # LOGGING\n",
    "        df_log = pd.DataFrame({'Step': step_log, 'Average Return': returns, '% Finished': finished, 'Crash Counter': crashed, 'Stuck Counter': stucked, 'Avg Steps/Episode': steped, 'Loss log': log_loss})\n",
    "        #df_log.to_csv(f\"logs/04-stateChange/{description}_comb-{combination+1}-run-{run+1}.csv\", index=None, header=True)\n",
    "\n",
    "        # CLEAR MEMORY\n",
    "        #del(agent)\n",
    "        del(session)\n",
    "        del(df_log)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import create_policy_eval_video\n",
    "\n",
    "# Reset the environment.\n",
    "time_step = train_py_env.reset()\n",
    "train_py_env.set_mode(1)\n",
    "train_py_env.set_size(maze_size)\n",
    "\n",
    "\n",
    "# Gera video da politica do agente treinado até então\n",
    "create_policy_eval_video(agent.policy, f\"trained-agent-{description}\", train_env, train_py_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-drone-v03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
