{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards Updated: {'destroyed': -10.0, 'stuck': -6.0, 'reached': 10.0, 'standard': -1.0}\n",
      "Rewards Updated: {'destroyed': -10.0, 'stuck': -6.0, 'reached': 10.0, 'standard': -1.0}\n",
      "WARNING:tensorflow:From /home/naski/Documents/dev/maze_drone_v02/maze-drone-v03/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "Rewards: {'destroyed': -10.0, 'stuck': -6.0, 'reached': 10.0, 'standard': -1.0}\n",
      "WARNING:tensorflow:From /home/naski/Documents/dev/maze_drone_v02/maze-drone-v03/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "\n",
    "from resources import build_agent, TrainingSession\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "min_combination = 1\n",
    "min_run = 1\n",
    "max_run = 1\n",
    "\n",
    "\n",
    "num_iterations = 5_000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 64  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "#replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-2  # @param {type:\"number\"}\n",
    "log_interval = 100  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 100  # @param {type:\"integer\"}\n",
    "\n",
    "# Agent fully connected layer params \n",
    "fc_layer_params = (200,) \n",
    "\n",
    "# File's name\n",
    "description = \"11\"\n",
    "\n",
    "rewards = []\n",
    "rewards.append({\n",
    "    'destroyed': -10.,\n",
    "    'stuck': - 6.,\n",
    "    'reached': 10.,\n",
    "    'standard': -1.\n",
    "})\n",
    "rewards.append({\n",
    "    'destroyed': -10.,\n",
    "    'stuck': -11.,\n",
    "    'reached': 10.,\n",
    "    'standard': -1.\n",
    "})\n",
    "rewards.append({\n",
    "    'destroyed': -10.,\n",
    "    'stuck': -15.,\n",
    "    'reached': 10.,\n",
    "    'standard': -1.\n",
    "})\n",
    "\n",
    "sys.path.append('/home/naski/Documents/dev/maze_drone_v02')\n",
    "import gym_maze # Esta linha precisa estar ap√≥s o PATH\n",
    "\n",
    "# Importing custom environment\n",
    "env_name = 'maze-v0'\n",
    "env = suite_gym.load(env_name)\n",
    "\n",
    "# Testing\n",
    "env.reset()\n",
    "\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "# Converts environments, originally in pure Python, to tensors (using a wrapper)\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CREATING/RESETING THE AGENT\n",
    "agent = build_agent(fc_layer_params, env, learning_rate, train_env)\n",
    "agent.initialize()\n",
    "\n",
    "# GENERATE TRAINING SESSION\n",
    "session = TrainingSession(description, env_name, rewards[0], agent, collect_steps_per_iteration, \n",
    "                        1, eval_interval, replay_buffer_max_length, num_eval_episodes)\n",
    "\n",
    "# TRAINING\n",
    "step_log, returns, finished, crashed, stucked, steped, _, replay_buffer, _ = session.train()\n",
    "\n",
    "# CLEAR MEMORY\n",
    "del(session)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._optimizer.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================== After first training:\n",
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=2500>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=2500>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=2500>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=2500>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=2500>\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================== After first training:\")\n",
    "\n",
    "print(agent.train_step_counter)\n",
    "print(agent._optimizer.learning_rate)\n",
    "\n",
    "checkpoint_dir = './checkpoint/phase-1-crash'\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=agent.train_step_counter\n",
    ")\n",
    "\n",
    "print(agent.train_step_counter)\n",
    "\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "    \n",
    "print(agent.train_step_counter)\n",
    "\n",
    "print(agent.train_step_counter)\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "print(agent.train_step_counter)\n",
    "\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=agent.train_step_counter\n",
    ")\n",
    "\n",
    "print(agent.train_step_counter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n"
     ]
    }
   ],
   "source": [
    "print(agent._optimizer.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._optimizer.learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n"
     ]
    }
   ],
   "source": [
    "print(agent._optimizer.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1  | Run 1\n",
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=2500>\n",
      "Rewards Updated: {'destroyed': -10.0, 'stuck': -6.0, 'reached': 10.0, 'standard': -1.0}\n",
      "Rewards Updated: {'destroyed': -10.0, 'stuck': -6.0, 'reached': 10.0, 'standard': -1.0}\n",
      "Rewards: {'destroyed': -10.0, 'stuck': -6.0, 'reached': 10.0, 'standard': -1.0}\n",
      "step = 3000\n",
      "step = 4000\n",
      "step = 5000\n",
      "step = 6000\n",
      "step = 7000\n"
     ]
    }
   ],
   "source": [
    "for combination in range(min_combination-1, min_combination):\n",
    "\n",
    "    for run in range(min_run-1, max_run):\n",
    "        print('Combination', combination + 1, ' | Run', run + 1)\n",
    "        train_checkpointer = common.Checkpointer(\n",
    "            ckpt_dir=checkpoint_dir,\n",
    "            max_to_keep=1,\n",
    "            agent=agent,\n",
    "            policy=agent.policy,\n",
    "            replay_buffer=replay_buffer,\n",
    "            global_step=agent.train_step_counter\n",
    "        )\n",
    "        agent._optimizer.learning_rate = learning_rate\n",
    "        print(agent.train_step_counter)\n",
    "\n",
    "\n",
    "        # GENERATE TRAINING SESSION\n",
    "        session = TrainingSession(description, env_name, rewards[combination], agent, collect_steps_per_iteration, \n",
    "                                num_iterations, eval_interval, replay_buffer_max_length, num_eval_episodes)\n",
    "        \n",
    "        # TRAINING\n",
    "        step_log, returns, finished, crashed, stucked, steped, log_loss, _, _ = session.train(without_wall_training=True)\n",
    "\n",
    "        # LOGGING\n",
    "        df_log = pd.DataFrame({'Step': step_log, 'Average Return': returns, '% Finished': finished, 'Crash Counter': crashed, 'Stuck Counter': stucked, 'Avg Steps/Episode': steped, 'Loss log': log_loss})\n",
    "        df_log.to_csv(f\"logs/02-stuck-improving/{description}_comb-{combination+1}-run-{run+1}.csv\", index=None, header=True)\n",
    "\n",
    "        # CLEAR MEMORY\n",
    "        #del(agent)\n",
    "        del(session)\n",
    "        del(df_log)\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-drone-v03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
