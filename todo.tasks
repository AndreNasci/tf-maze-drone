Rendering:
    ✔ Gerar imagem do drone @done (5/27/2024, 6:22:40 PM)
    ✔ Função que atualiza imagem a cada movimento @done (5/27/2024, 6:22:25 PM)
    ✔ Modificar o código para exibir o environment por meio da função render @done (5/27/2024, 6:22:33 PM)

    Pensar se vale a pena converter o código para Pygame

TF-Agents Compatibility:
    ✔ time_step_spec @done (5/27/2024, 4:01:18 PM)
    ✔ action_spec @done (5/27/2024, 4:01:24 PM)
    ✔ step return format @done (5/27/2024, 4:01:26 PM)

To do:
    ☐ Tornar tamanho do maze variável 
    ☐ Limpar código desnecessário
    ☐ Comparar estruturas das redes neurais usadas nos diferentes tutoriais

Entender:
    ☐ Loss
    ✔ Average Return @done (5/29/2024, 4:25:24 PM)
    ☐ Criação do agente e configurações da rede neural
    ☐ Metrics and evaluation
    ☐ DynamicStepDriver (collect_steps_per_iteration)
    ☐ agent.train_step_counter
    ☐ Construção da rede neural, parâmetros

Testes:
    ✔ Remover métodos action_spec e observation_spec e ver se ainda funciona @done (5/28/2024, 5:48:56 PM)
    ☐ Testar REINFORCE ou PPO
    ☐ Grafo como entrada para Rede Neural? (SLAM)
    ✘ Reverb @cancelled (5/29/2024, 4:25:36 PM)
    ☐ Alterar a função loss
    ☐ Testar PyDriver para coleta de mais métricas
    
Features:
    ✔ Add a step limit @done (5/28/2024, 5:48:29 PM)
    ✘ Change buffer @cancelled (5/29/2024, 4:21:57 PM)
    
    

Alt+d - complete task
Alt+c - cancel task
@ tag - makes a tag


avg _return calcular a média dos rewards
como não tem um máximo que o modelo sempre atinge(depende
da distância), talvez seja necessário rever a política de 
reward para ser possível avaliar melhor o desempenho do
modelo

Agents: ReinforceAgent, DQNAgent

Metrics:
    - TensorBoard: average episode length **
    - Talvez passar episode length via {info}, no retorno de step
    coletar com algum observer

afinal ele treina a cada coleta?
só faz um sampling de amostras diferentes? 

Possible issues:
size of buffer
learning rate (adam)
reward policy
maze is too big and complex (go step by step)
it may need more wins to learn the correlation with distance

